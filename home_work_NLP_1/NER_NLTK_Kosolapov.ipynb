{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Извлечение именованных сущностей и фактов**"
      ],
      "metadata": {
        "id": "1Qz_eOCWk96v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Извлечение объектов и фактов из текстов – это часть NLP\n",
        "Выделение именованных сущностей (named entity recognition) - одна из ключевых задач\n",
        "извлечения информации, извлечения структурированных данных из неструктурированных документов. Ее суть - найти в тексте названия, идентификаторы объектов определенного типа, это могут быть фестивали, продукты, биологические объекты, протеины\n",
        "и т.д."
      ],
      "metadata": {
        "id": "WNUvrdnQlM4g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Именованная сущность\n",
        "(named entity) - объект\n",
        "определенного типа, имеющий имя, название или идентификатор."
      ],
      "metadata": {
        "id": "5BtH6o1bl8Jc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Примеры"
      ],
      "metadata": {
        "id": "ZwJCot35mFEy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "english_text = ''' I want a person available 7 days and with prompt response all most every time. Only Indian freelancer need I need PHP developer who have strong experience in Laravel and Codeigniter framework for daily 4 hours. I need this work by Monday 27th Jan. should be free from plagiarism . \n",
        "Need SAP FICO consultant for support project needs to be work on 6 months on FI AREAWe.  Want a same site to be created as the same as this https://www.facebook.com/?ref=logo, please check the site before contacting to me and i want this site to be ready in 10 days. They will be ready at noon tomorrow .'''\n",
        "\n",
        "russian_text = '''Власти Москвы выделили 110 млрд рублей на поддержку населения, системы здравоохранения и городского хозяйства. Об этом сообщается на сайте мэра столицы https://www.sobyanin.ru/ в пятницу, 1 мая. По адресу Алтуфьевское шоссе д.51 (основной вид разрешенного использования: производственная деятельность, склады) размещен МПЗ? Подпоручик Киже управляя автомобилем ВАЗ2107 перевозил автомат АК47 с целью ограбления банка ВТБ24, как следует из записей. \n",
        "Взыскать c индивидуального предпринимателя Иванова Костантипа Петровича дата рождения 10 января 1970 года, проживающего по адресу город Санкт-Петербург, ул. Крузенштерна, дом 5/1А 8 000 (восемь тысяч) рублей 00 копеек гос. пошлины в пользу бюджета РФ Жители требуют незамедлительной остановки МПЗ и его вывода из района. Решение было принято по поручению мэра города Сергея Собянина в связи с ограничениями из-за коронавируса.'''"
      ],
      "metadata": {
        "id": "Q6hJp_17L4vA"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "NLTK"
      ],
      "metadata": {
        "id": "_W2VodGQ1Fbl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "NLTK это классическая библиотека для обработки естественного языка, она проста в использовании, не требует длительного изучения и выполняет 99% задач, которые могут возникнуть при решении студенческих задач."
      ],
      "metadata": {
        "id": "b1wMN7NFzvYq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "#pos_tag - определение части речи для каждошо слова\n",
        "#ne_chunk - извлечение именованных сущностей\n",
        "for sent in nltk.sent_tokenize(english_text):\n",
        "   for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sent))):\n",
        "      if hasattr(chunk, 'label'):\n",
        "         print(chunk)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Kbe727qlE4p",
        "outputId": "56a506d2-cfe4-4035-99e0-a62b936ea568"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(GPE Indian/JJ)\n",
            "(ORGANIZATION PHP/NNP)\n",
            "(GPE Laravel/NNP)\n",
            "(PERSON Need/NNP)\n",
            "(ORGANIZATION SAP/NNP)\n",
            "(ORGANIZATION FI/NNP)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Spacy"
      ],
      "metadata": {
        "id": "D4jrmmPh1RnV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Spacy это Python-библиотека с открытым исходным кодом для обработки естественного языка, она издается под лицензией MIT (!), ее создали и развивают Мэтью Ганнибал и Инес Монтани, основатели компании-разработчика Explosion."
      ],
      "metadata": {
        "id": "v28_NC_az4BS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "#model_sp = en_core_web_lg.load()\n",
        "for ent in nlp(english_text).ents:\n",
        "  print(ent.text.strip(), ent.label_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BmGflLMylMXM",
        "outputId": "b83b91e2-1c74-4ae3-bdd8-9dab1d86a03b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7 days DATE\n",
            "Indian NORP\n",
            "PHP ORG\n",
            "Laravel GPE\n",
            "4 hours TIME\n",
            "Monday 27th DATE\n",
            "Jan. DATE\n",
            "6 months DATE\n",
            "10 days DATE\n",
            "noon TIME\n",
            "tomorrow DATE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Flair"
      ],
      "metadata": {
        "id": "hz6vPwbZ12V5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Flair предлагает гораздо более глубокое погружение в предметную область, библиотека создана, фактически, для решения исследовательских задач, документация неплохая, но с некоторыми провалами, есть интеграция с большим количеством других библиотек, понятный, логичный и читаемый код."
      ],
      "metadata": {
        "id": "B2ArjUkcz86w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install flair"
      ],
      "metadata": {
        "id": "QYJAUWww0MSE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import flair\n",
        "from flair.models import SequenceTagger\n",
        "tagger = SequenceTagger.load('ner')\n",
        "from flair.data import Sentence\n",
        "s = Sentence(english_text)\n",
        "tagger.predict(s)\n",
        "for entity in s.get_spans('ner'):\n",
        "    print(entity)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cNB2-GQK13FY",
        "outputId": "c117cf0c-edb3-4e03-dc2a-be4e5977e9f9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-11-24 18:08:06,047 loading file /root/.flair/models/ner-english/4f4cdab26f24cb98b732b389e6cebc646c36f54cfd6e0b7d3b90b25656e4262f.8baa8ae8795f4df80b28e7f7b61d788ecbb057d1dc85aacb316f1bd02837a4a4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/huggingface_hub/file_download.py:595: FutureWarning: `cached_download` is the legacy way to download files from the HF hub, please consider upgrading to `hf_hub_download`\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-11-24 18:08:08,634 SequenceTagger predicts: Dictionary with 20 tags: <unk>, O, S-ORG, S-MISC, B-PER, E-PER, S-LOC, B-ORG, E-ORG, I-PER, S-PER, B-MISC, I-MISC, E-MISC, I-ORG, B-LOC, E-LOC, I-LOC, <START>, <STOP>\n",
            "Span[17:18]: \"Indian\" → MISC (0.9972)\n",
            "Span[29:30]: \"Laravel\" → ORG (0.2937)\n",
            "Span[31:32]: \"Codeigniter\" → MISC (0.6584)\n",
            "Span[67:69]: \"FI AREAWe\" → MISC (0.8724)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NLTK**\n",
        "\n",
        "https://www.nltk.org/ - официальная документация"
      ],
      "metadata": {
        "id": "UW0uqjL547KZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DOYNJqsP45su",
        "outputId": "5bc12aa2-27fc-41ec-c8c0-2d3a587ff2fe"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download(\"stopwords\")\n",
        "from nltk.corpus import stopwords\n",
        "from string import punctuation\n",
        "english_stopwords = stopwords.words(\"english\")\n",
        "english_stopwords"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YmQ6bDsH5IPG",
        "outputId": "66c790cd-7538-4e3d-cca5-f89bc8005549"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i',\n",
              " 'me',\n",
              " 'my',\n",
              " 'myself',\n",
              " 'we',\n",
              " 'our',\n",
              " 'ours',\n",
              " 'ourselves',\n",
              " 'you',\n",
              " \"you're\",\n",
              " \"you've\",\n",
              " \"you'll\",\n",
              " \"you'd\",\n",
              " 'your',\n",
              " 'yours',\n",
              " 'yourself',\n",
              " 'yourselves',\n",
              " 'he',\n",
              " 'him',\n",
              " 'his',\n",
              " 'himself',\n",
              " 'she',\n",
              " \"she's\",\n",
              " 'her',\n",
              " 'hers',\n",
              " 'herself',\n",
              " 'it',\n",
              " \"it's\",\n",
              " 'its',\n",
              " 'itself',\n",
              " 'they',\n",
              " 'them',\n",
              " 'their',\n",
              " 'theirs',\n",
              " 'themselves',\n",
              " 'what',\n",
              " 'which',\n",
              " 'who',\n",
              " 'whom',\n",
              " 'this',\n",
              " 'that',\n",
              " \"that'll\",\n",
              " 'these',\n",
              " 'those',\n",
              " 'am',\n",
              " 'is',\n",
              " 'are',\n",
              " 'was',\n",
              " 'were',\n",
              " 'be',\n",
              " 'been',\n",
              " 'being',\n",
              " 'have',\n",
              " 'has',\n",
              " 'had',\n",
              " 'having',\n",
              " 'do',\n",
              " 'does',\n",
              " 'did',\n",
              " 'doing',\n",
              " 'a',\n",
              " 'an',\n",
              " 'the',\n",
              " 'and',\n",
              " 'but',\n",
              " 'if',\n",
              " 'or',\n",
              " 'because',\n",
              " 'as',\n",
              " 'until',\n",
              " 'while',\n",
              " 'of',\n",
              " 'at',\n",
              " 'by',\n",
              " 'for',\n",
              " 'with',\n",
              " 'about',\n",
              " 'against',\n",
              " 'between',\n",
              " 'into',\n",
              " 'through',\n",
              " 'during',\n",
              " 'before',\n",
              " 'after',\n",
              " 'above',\n",
              " 'below',\n",
              " 'to',\n",
              " 'from',\n",
              " 'up',\n",
              " 'down',\n",
              " 'in',\n",
              " 'out',\n",
              " 'on',\n",
              " 'off',\n",
              " 'over',\n",
              " 'under',\n",
              " 'again',\n",
              " 'further',\n",
              " 'then',\n",
              " 'once',\n",
              " 'here',\n",
              " 'there',\n",
              " 'when',\n",
              " 'where',\n",
              " 'why',\n",
              " 'how',\n",
              " 'all',\n",
              " 'any',\n",
              " 'both',\n",
              " 'each',\n",
              " 'few',\n",
              " 'more',\n",
              " 'most',\n",
              " 'other',\n",
              " 'some',\n",
              " 'such',\n",
              " 'no',\n",
              " 'nor',\n",
              " 'not',\n",
              " 'only',\n",
              " 'own',\n",
              " 'same',\n",
              " 'so',\n",
              " 'than',\n",
              " 'too',\n",
              " 'very',\n",
              " 's',\n",
              " 't',\n",
              " 'can',\n",
              " 'will',\n",
              " 'just',\n",
              " 'don',\n",
              " \"don't\",\n",
              " 'should',\n",
              " \"should've\",\n",
              " 'now',\n",
              " 'd',\n",
              " 'll',\n",
              " 'm',\n",
              " 'o',\n",
              " 're',\n",
              " 've',\n",
              " 'y',\n",
              " 'ain',\n",
              " 'aren',\n",
              " \"aren't\",\n",
              " 'couldn',\n",
              " \"couldn't\",\n",
              " 'didn',\n",
              " \"didn't\",\n",
              " 'doesn',\n",
              " \"doesn't\",\n",
              " 'hadn',\n",
              " \"hadn't\",\n",
              " 'hasn',\n",
              " \"hasn't\",\n",
              " 'haven',\n",
              " \"haven't\",\n",
              " 'isn',\n",
              " \"isn't\",\n",
              " 'ma',\n",
              " 'mightn',\n",
              " \"mightn't\",\n",
              " 'mustn',\n",
              " \"mustn't\",\n",
              " 'needn',\n",
              " \"needn't\",\n",
              " 'shan',\n",
              " \"shan't\",\n",
              " 'shouldn',\n",
              " \"shouldn't\",\n",
              " 'wasn',\n",
              " \"wasn't\",\n",
              " 'weren',\n",
              " \"weren't\",\n",
              " 'won',\n",
              " \"won't\",\n",
              " 'wouldn',\n",
              " \"wouldn't\"]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "russian_stopwords = stopwords.words(\"russian\")\n",
        "russian_stopwords"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nCzfuCTw5Tkh",
        "outputId": "155595b9-97c3-4360-8766-272ddb034758"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['и',\n",
              " 'в',\n",
              " 'во',\n",
              " 'не',\n",
              " 'что',\n",
              " 'он',\n",
              " 'на',\n",
              " 'я',\n",
              " 'с',\n",
              " 'со',\n",
              " 'как',\n",
              " 'а',\n",
              " 'то',\n",
              " 'все',\n",
              " 'она',\n",
              " 'так',\n",
              " 'его',\n",
              " 'но',\n",
              " 'да',\n",
              " 'ты',\n",
              " 'к',\n",
              " 'у',\n",
              " 'же',\n",
              " 'вы',\n",
              " 'за',\n",
              " 'бы',\n",
              " 'по',\n",
              " 'только',\n",
              " 'ее',\n",
              " 'мне',\n",
              " 'было',\n",
              " 'вот',\n",
              " 'от',\n",
              " 'меня',\n",
              " 'еще',\n",
              " 'нет',\n",
              " 'о',\n",
              " 'из',\n",
              " 'ему',\n",
              " 'теперь',\n",
              " 'когда',\n",
              " 'даже',\n",
              " 'ну',\n",
              " 'вдруг',\n",
              " 'ли',\n",
              " 'если',\n",
              " 'уже',\n",
              " 'или',\n",
              " 'ни',\n",
              " 'быть',\n",
              " 'был',\n",
              " 'него',\n",
              " 'до',\n",
              " 'вас',\n",
              " 'нибудь',\n",
              " 'опять',\n",
              " 'уж',\n",
              " 'вам',\n",
              " 'ведь',\n",
              " 'там',\n",
              " 'потом',\n",
              " 'себя',\n",
              " 'ничего',\n",
              " 'ей',\n",
              " 'может',\n",
              " 'они',\n",
              " 'тут',\n",
              " 'где',\n",
              " 'есть',\n",
              " 'надо',\n",
              " 'ней',\n",
              " 'для',\n",
              " 'мы',\n",
              " 'тебя',\n",
              " 'их',\n",
              " 'чем',\n",
              " 'была',\n",
              " 'сам',\n",
              " 'чтоб',\n",
              " 'без',\n",
              " 'будто',\n",
              " 'чего',\n",
              " 'раз',\n",
              " 'тоже',\n",
              " 'себе',\n",
              " 'под',\n",
              " 'будет',\n",
              " 'ж',\n",
              " 'тогда',\n",
              " 'кто',\n",
              " 'этот',\n",
              " 'того',\n",
              " 'потому',\n",
              " 'этого',\n",
              " 'какой',\n",
              " 'совсем',\n",
              " 'ним',\n",
              " 'здесь',\n",
              " 'этом',\n",
              " 'один',\n",
              " 'почти',\n",
              " 'мой',\n",
              " 'тем',\n",
              " 'чтобы',\n",
              " 'нее',\n",
              " 'сейчас',\n",
              " 'были',\n",
              " 'куда',\n",
              " 'зачем',\n",
              " 'всех',\n",
              " 'никогда',\n",
              " 'можно',\n",
              " 'при',\n",
              " 'наконец',\n",
              " 'два',\n",
              " 'об',\n",
              " 'другой',\n",
              " 'хоть',\n",
              " 'после',\n",
              " 'над',\n",
              " 'больше',\n",
              " 'тот',\n",
              " 'через',\n",
              " 'эти',\n",
              " 'нас',\n",
              " 'про',\n",
              " 'всего',\n",
              " 'них',\n",
              " 'какая',\n",
              " 'много',\n",
              " 'разве',\n",
              " 'три',\n",
              " 'эту',\n",
              " 'моя',\n",
              " 'впрочем',\n",
              " 'хорошо',\n",
              " 'свою',\n",
              " 'этой',\n",
              " 'перед',\n",
              " 'иногда',\n",
              " 'лучше',\n",
              " 'чуть',\n",
              " 'том',\n",
              " 'нельзя',\n",
              " 'такой',\n",
              " 'им',\n",
              " 'более',\n",
              " 'всегда',\n",
              " 'конечно',\n",
              " 'всю',\n",
              " 'между']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_data = \"The First sentence is about Python. The Second: about Django. You can learn Python,Django and Data Ananlysis here. \"\n",
        "nltk_tokens = nltk.sent_tokenize(sentence_data)\n",
        "print(nltk_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5CygITy16ug9",
        "outputId": "6251b2a7-359f-44de-f954-64410aa0934a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The First sentence is about Python.', 'The Second: about Django.', 'You can learn Python,Django and Data Ananlysis here.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_data = \"It originated from the idea that there are readers who prefer learning new skills from the comforts of their drawing rooms\"\n",
        "nltk_tokens = nltk.word_tokenize(word_data)\n",
        "print(nltk_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M5Bo_-zp6xAY",
        "outputId": "71f4d480-4080-4b5f-bd50-9c34ab4c4d0a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['It', 'originated', 'from', 'the', 'idea', 'that', 'there', 'are', 'readers', 'who', 'prefer', 'learning', 'new', 'skills', 'from', 'the', 'comforts', 'of', 'their', 'drawing', 'rooms']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_sentence = [w for w in nltk_tokens if not w.lower() in english_stopwords]\n",
        "filtered_sentence"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ChfPqafr7U1g",
        "outputId": "03e381ea-3aa0-42ad-c6c0-22ea0aec7d32"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['originated',\n",
              " 'idea',\n",
              " 'readers',\n",
              " 'prefer',\n",
              " 'learning',\n",
              " 'new',\n",
              " 'skills',\n",
              " 'comforts',\n",
              " 'drawing',\n",
              " 'rooms']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Стемминг и Лемматизация"
      ],
      "metadata": {
        "id": "0GamY1M57wmn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "e_words= [\"wait\", \"waiting\", \"waited\", \"waits\"]\n",
        "ps = PorterStemmer()\n",
        "for w in e_words:\n",
        "    rootWord=ps.stem(w)\n",
        "    print(rootWord)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3eua2WhG7t0R",
        "outputId": "593e9d8f-5e65-49d8-8f2b-62452639606d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "wait\n",
            "wait\n",
            "wait\n",
            "wait\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# создаем модель лемматизатора\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "text = \"studies studying cries cry\"\n",
        "\n",
        "# сначала токенезируем текст\n",
        "tokenization = nltk.word_tokenize(text)\n",
        "\n",
        "for w in tokenization:\n",
        "    print(\"Lemma for {} is {}\".format(w, wordnet_lemmatizer.lemmatize(w)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S4l0IjkK8AKr",
        "outputId": "4aeaef4b-5011-4e71-c509-494f4a365cc4"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lemma for studies is study\n",
            "Lemma for studying is studying\n",
            "Lemma for cries is cry\n",
            "Lemma for cry is cry\n"
          ]
        }
      ]
    }
  ]
}